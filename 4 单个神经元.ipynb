{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 激活函数—加入非线性因素，解决线性模型缺陷"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 500x1000 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.linspace(-10, 10, 1000)\n",
    "# Sigmoid函数\n",
    "sigmoid = 1/(1+np.exp(-x))\n",
    "\n",
    "# Tanh函数\n",
    "tanh = (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))\n",
    "\n",
    "# Relu函数\n",
    "relu = np.where(x > 0, x, 0)\n",
    "\n",
    "# Softplus函数\n",
    "softplus = np.log(1+np.exp(x))\n",
    "\n",
    "# Noisy relus函数\n",
    "noisy_relu = np.where(x > 0, x+np.random.normal(loc=0, scale=np.std(x)), 0)\n",
    "\n",
    "# Leaky relus函数\n",
    "leaky_relu = np.where(x > 0, x, 0.1*x)\n",
    "\n",
    "# Elus函数\n",
    "elus = np.where(x>0, x, (0.1 * np.exp(x)-1))\n",
    "\n",
    "# Swish函数\n",
    "swish = (x * sigmoid)\n",
    "\n",
    "loss_func = [sigmoid, tanh, relu, softplus, noisy_relu, leaky_relu, elus, swish]\n",
    "loss_func_tf=['sigmoid',\n",
    "              'tanh',\n",
    "              'relu',\n",
    "              'softplus',\n",
    "              'noisy_relu',\n",
    "              'leaky_relu',\n",
    "              'elus',\n",
    "              'swish']\n",
    "\n",
    "fig, axes = plt.subplots(4, 2, figsize=(5, 10))\n",
    "fig.tight_layout()\n",
    "\n",
    "for loss_func, loss_func_tf, ax in zip(loss_func, loss_func_tf, axes.ravel()):\n",
    "    ax.set_title(loss_func_tf)\n",
    "    ax.grid()\n",
    "    ax.plot(x, loss_func)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.grid()\n",
    "plt.plot(x, sigmoid, label='sigmoid')\n",
    "plt.plot(x, tanh, label='tanh')\n",
    "plt.plot(x, relu, label='relu')\n",
    "plt.plot(x, softplus, label='softplus')\n",
    "plt.plot(x, leaky_relu, label='leaky_relu')\n",
    "plt.plot(x, elus, label='elus')\n",
    "plt.plot(x, swish, label='swish')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 softmax算法与损失函数的综合应用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.1 交叉熵实验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-3f37fc0836fd>:17: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "scaled=[[0.01791432 0.00399722 0.97808844]\n",
      " [0.04980332 0.04506391 0.90513283]]\n",
      "scaled2=[[0.21747023 0.21446465 0.56806517]\n",
      " [0.2300214  0.22893383 0.5410447 ]]\n",
      "rel1=[0.02215516 3.0996735 ]\n",
      "rel2=[0.56551915 1.4743223 ]\n",
      "rel3=[0.02215518 3.0996735 ]\n"
     ]
    }
   ],
   "source": [
    "# 假设有一个标签labels和一个网络输出值logits\n",
    "# 两次softmax实验: 将输出值logits分别进行一次和两次softmax，观察两次的区别和意义\n",
    "# 观察交叉熵：将上一步中的两个值分别进行softmax_corss_entropy_with_logits,观察它们的区别\n",
    "# 自建公式实验：将两次softmax的值放入自建组合的公式中得到正确的值\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# labels and logits\n",
    "labels = [[0, 0, 1], [0, 1, 0]]\n",
    "logits = [[2, 0.5, 6],\n",
    "          [0.1, 0, 3]]\n",
    "\n",
    "logits_scaled = tf.nn.softmax(logits) # 真实转化的softmax值\n",
    "logits_scaled2 = tf.nn.softmax(logits_scaled) # 经过第二次softmax后，分布概率会有变化\n",
    "\n",
    "# 正确的方式\n",
    "result1 = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n",
    "\n",
    "# 传入softmax_cross_entropyIwith_logits的logits不需要进行softmax\n",
    "# 如果将softmax变换完的值放进去，就相当于算第二次softmax的loss，所以会出错\n",
    "result2 = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits_scaled)\n",
    "\n",
    "# 对于已经用softmax转换过的scaled，可以自建一个loss函数\n",
    "result3 = -tf.reduce_sum(labels*tf.log(logits_scaled), axis=1)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print('scaled={}'.format(sess.run(logits_scaled)))\n",
    "    print('scaled2={}'.format(sess.run(logits_scaled2)))\n",
    "    print('rel1={}'.format(sess.run(result1)))\n",
    "    print('rel2={}'.format(sess.run(result2)))\n",
    "    print('rel3={}'.format(sess.run(result3)))\n",
    "\n",
    "# logits中的值原本加和是大于1的，经过softmax后，总和变成了1\n",
    "# 样本中第一个是跟标签分类相符的， 第二与标签分类不符，所以第一个的交叉熵比较小，第二个比较大\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 4.5.2 one_hot实验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rel4=[2.1721554 2.7696736]\n"
     ]
    }
   ],
   "source": [
    "# 输入的标签也可以不是标准的one_hot\n",
    "# 下面用一组总和是1，但数组中每个值不等于0或1的数组来代替标签\n",
    "\n",
    "# 对非one_hot编码为标签的数据进行交叉熵的计算，比较其与one_hot编码之间的差别\n",
    "\n",
    "labels = [[0.4, 0.1, 0.5],\n",
    "          [0.3, 0.6, 0.1]]\n",
    "result4 = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n",
    "with tf.Session() as sess:\n",
    "    print('rel4={}'.format(sess.run(result4))) \n",
    "    # 对比rel1发现，正确分类的交叉熵和错误分类的交叉熵，二者的错误没有标准one_hot明显"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.3 sparse交叉熵的使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rel5=[0.02215516 3.0996735 ]\n"
     ]
    }
   ],
   "source": [
    "# 使用sparse_softmax_cross_with_logits函数对非one_hot的标签进行交叉熵计算，比较其与one_hot标签的区别\n",
    "# sparse_softmax_cross_with_logits需要使用非one_hot的标签\n",
    "\n",
    "# sparse标签\n",
    "labels =[2,1] # 表明labels中总共分为3个类： 0、1和2\n",
    "              # 2、1分别对应one_hot编码中的001与010\n",
    "result5 = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n",
    "with tf.Session() as sess:\n",
    "    print('rel5={}'.format(sess.run(result5))) # 与rel1的结果相同"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算loss值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=3.121828556060791\n"
     ]
    }
   ],
   "source": [
    "loss=tf.reduce_sum(result1)\n",
    "with tf.Session() as sess:\n",
    "    print('loss={}'.format(sess.run(loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss2=3.12182879447937\n"
     ]
    }
   ],
   "source": [
    "labels=[[0,0,1],[0,1,0]]\n",
    "loss2 = -tf.reduce_sum(labels * tf.log(logits_scaled))\n",
    "with tf.Session() as sess:\n",
    "    print('loss2={}'.format(sess.run(loss2))) # 与loss值相同"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.6 梯度下降—让模型逼近最小偏差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6.1 梯度下降的作用及分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 常用的梯度下降方法\n",
    "# 批量梯度下降\n",
    "# 随机梯度下降\n",
    "# 小批量梯度下降"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6.2 TensorFlow中的梯度下降函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.train.GradientDescentOptimizer()\n",
    "# tf.train.AdadeltaOptimizer()\n",
    "# tf.train.AdagradOptimizer()\n",
    "# tf.train.MomentumOptimizer()\n",
    "# tf.train.AdamOptimizer()\n",
    "# tf.train.FtrlOptimizer()\n",
    "# tf.train.RMSPropOptimzer()\n",
    "\n",
    "# 在训练过程中，先实例化一个优化函数，并基于一定的学习率进行梯度优化训练\n",
    "# 然后使用一个minimize()的操作，里面传入损失值节点loss\n",
    "# 再启动一个外层的循环，优化器就会按照循环的次数沿着loss的最小值的方向优化参数了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6.3 退化学习率—在训练的速度与精度之间找到平衡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每个优化器的第一个参数learning_rate就是代表学习率\n",
    "# 设置学习率的大小，是在精度和速度之间找到一个平衡:\n",
    "# 如果学习率的值比较大，则训练的速度会提升，但结果精度不够\n",
    "# 如果学习率的值比较小，精度虽然提升了，但训练会耗费太多时间\n",
    "\n",
    "# 退化学习率在训练刚开始时使用大的学习率加快速度，训练到一定程度后使用小的学习率来提高精度\n",
    "\n",
    "# 学习率的衰减是由global_step和decay_steps来决定的\n",
    "# def exponential_decay(learning_rate,global_step,decay_steps,decay_rate, staircase=False, name=None):\n",
    "#     decayed_learning_rate = learning_rate * decay_rate ^ (global_step / decay_steps)\n",
    "# staircase值默认为False，当为True时，将没有衰减功能\n",
    "\n",
    "# learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, 100000, 0.96)\n",
    "# 当前迭代到global_step步，学习率每一步都按照每100000步缩小到0.96的速度衰减\n",
    "\n",
    "# 有时还需要对已经训练好的模型进行微调，可以指定不同层使用不同的学习率\n",
    "# 增大批次处理样本的数量也可以起到退化学习率的作用，但这种方法要求训练时的最小批次要与实际应用中的最小批次一致\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6.4 退化学习率用法举例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "0.1\n",
      "1 0.1\n",
      "2 0.09791484\n",
      "3 0.09688862\n",
      "4 0.095873155\n",
      "5 0.095873155\n",
      "6 0.09387404\n",
      "7 0.092890166\n",
      "8 0.09191661\n",
      "9 0.09095325\n",
      "10 0.089999996\n",
      "11 0.089999996\n",
      "12 0.08812335\n",
      "13 0.087199755\n",
      "14 0.08628584\n",
      "15 0.0853815\n",
      "16 0.08448663\n",
      "17 0.08448663\n",
      "18 0.08272495\n",
      "19 0.08272495\n",
      "20 0.08099999\n"
     ]
    }
   ],
   "source": [
    "# 定义一个学习率变量，将其衰减系数设置好，并设置好循环次数\n",
    "# 将每次迭代运算的次数与学习率打印出来，观察学习率按照次数退化的现象\n",
    "\n",
    "# 初始学习率为0.1， 令其以每10次衰减到0.9倍的速度进行退化\n",
    "import tensorflow as tf\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "initial_learning_rate=0.1\n",
    "learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step=global_step, \n",
    "                                           decay_steps=10, decay_rate=0.9)\n",
    "opt = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "add_global = global_step.assign_add(1) # 定义一个op，令global_step加1完成计步\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(sess.run(learning_rate))\n",
    "    for i in range(20):\n",
    "        g, rate = sess.run([add_global, learning_rate])\n",
    "        print(g, rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.7 初始化学习参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在定义学习参数时可以通过get_variable和Variable两个方式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.8 单个神经元的扩展—Maxout网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.8.1 Maxout介绍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maxout网络可以理解为单个神经元的扩展，主要是扩展单个神经元的激活函数\n",
    "# Maxout是将激活函数变成一个网络选择器，原理是将多个神经元并列地放在一起\n",
    "# 从它们的输出结果中找出最大的那个，代表对特征响应最敏感，然后取这个神经元参与后面的计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.8.2 用Maxout网络实现MNIST分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-13-187ae0cabbec>:9: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From E:\\anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From E:\\anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From E:\\anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From E:\\anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From E:\\anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From <ipython-input-13-187ae0cabbec>:22: calling reduce_max_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From E:\\anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From E:\\anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch: 0001 cost= 2.318497851\n",
      "Epoch: 0002 cost= 2.313091905\n",
      "Epoch: 0003 cost= 2.308083458\n",
      "Epoch: 0004 cost= 2.304259282\n",
      "Epoch: 0005 cost= 2.298465976\n",
      "Epoch: 0006 cost= 2.291202432\n",
      "Epoch: 0007 cost= 2.283648393\n",
      "Epoch: 0008 cost= 2.271758458\n",
      "Epoch: 0009 cost= 2.253412165\n",
      "Epoch: 0010 cost= 2.217015392\n",
      "Epoch: 0011 cost= 2.120678426\n",
      "Epoch: 0012 cost= 2.051884947\n",
      "Epoch: 0013 cost= 2.022253395\n",
      "Epoch: 0014 cost= 2.002433181\n",
      "Epoch: 0015 cost= 1.983266899\n",
      "Epoch: 0016 cost= 1.962011078\n",
      "Epoch: 0017 cost= 1.944497740\n",
      "Epoch: 0018 cost= 1.929740844\n",
      "Epoch: 0019 cost= 1.916376662\n",
      "Epoch: 0020 cost= 1.903558727\n",
      "Epoch: 0021 cost= 1.893336894\n",
      "Epoch: 0022 cost= 1.882650324\n",
      "Epoch: 0023 cost= 1.874086670\n",
      "Epoch: 0024 cost= 1.866016896\n",
      "Epoch: 0025 cost= 1.858472220\n",
      "Epoch: 0026 cost= 1.852371089\n",
      "Epoch: 0027 cost= 1.845590411\n",
      "Epoch: 0028 cost= 1.839368069\n",
      "Epoch: 0029 cost= 1.834008171\n",
      "Epoch: 0030 cost= 1.826216012\n",
      "Epoch: 0031 cost= 1.818086911\n",
      "Epoch: 0032 cost= 1.810931589\n",
      "Epoch: 0033 cost= 1.804032003\n",
      "Epoch: 0034 cost= 1.798335950\n",
      "Epoch: 0035 cost= 1.791266154\n",
      "Epoch: 0036 cost= 1.787970099\n",
      "Epoch: 0037 cost= 1.783665824\n",
      "Epoch: 0038 cost= 1.781483662\n",
      "Epoch: 0039 cost= 1.776648454\n",
      "Epoch: 0040 cost= 1.775420363\n",
      "Epoch: 0041 cost= 1.771823430\n",
      "Epoch: 0042 cost= 1.767913056\n",
      "Epoch: 0043 cost= 1.766593343\n",
      "Epoch: 0044 cost= 1.764217302\n",
      "Epoch: 0045 cost= 1.763638751\n",
      "Epoch: 0046 cost= 1.760194660\n",
      "Epoch: 0047 cost= 1.757903936\n",
      "Epoch: 0048 cost= 1.756821735\n",
      "Epoch: 0049 cost= 1.754335740\n",
      "Epoch: 0050 cost= 1.753615547\n",
      "Epoch: 0051 cost= 1.751500215\n",
      "Epoch: 0052 cost= 1.750386941\n",
      "Epoch: 0053 cost= 1.747905453\n",
      "Epoch: 0054 cost= 1.747306530\n",
      "Epoch: 0055 cost= 1.744575691\n",
      "Epoch: 0056 cost= 1.743621392\n",
      "Epoch: 0057 cost= 1.743056315\n",
      "Epoch: 0058 cost= 1.739433885\n",
      "Epoch: 0059 cost= 1.740062945\n",
      "Epoch: 0060 cost= 1.737474652\n",
      "Epoch: 0061 cost= 1.736773273\n",
      "Epoch: 0062 cost= 1.735532427\n",
      "Epoch: 0063 cost= 1.734519856\n",
      "Epoch: 0064 cost= 1.734645179\n",
      "Epoch: 0065 cost= 1.733643862\n",
      "Epoch: 0066 cost= 1.730293317\n",
      "Epoch: 0067 cost= 1.731019113\n",
      "Epoch: 0068 cost= 1.730946518\n",
      "Epoch: 0069 cost= 1.727683379\n",
      "Epoch: 0070 cost= 1.726251484\n",
      "Epoch: 0071 cost= 1.726648147\n",
      "Epoch: 0072 cost= 1.725600391\n",
      "Epoch: 0073 cost= 1.723831767\n",
      "Epoch: 0074 cost= 1.722458484\n",
      "Epoch: 0075 cost= 1.722356435\n",
      "Epoch: 0076 cost= 1.721717246\n",
      "Epoch: 0077 cost= 1.720028086\n",
      "Epoch: 0078 cost= 1.719250820\n",
      "Epoch: 0079 cost= 1.716764317\n",
      "Epoch: 0080 cost= 1.717901056\n",
      "Epoch: 0081 cost= 1.717180308\n",
      "Epoch: 0082 cost= 1.714047477\n",
      "Epoch: 0083 cost= 1.713958893\n",
      "Epoch: 0084 cost= 1.714455014\n",
      "Epoch: 0085 cost= 1.712293857\n",
      "Epoch: 0086 cost= 1.712913145\n",
      "Epoch: 0087 cost= 1.711484825\n",
      "Epoch: 0088 cost= 1.710341662\n",
      "Epoch: 0089 cost= 1.708792971\n",
      "Epoch: 0090 cost= 1.708137713\n",
      "Epoch: 0091 cost= 1.708337587\n",
      "Epoch: 0092 cost= 1.707609887\n",
      "Epoch: 0093 cost= 1.706266683\n",
      "Epoch: 0094 cost= 1.705510516\n",
      "Epoch: 0095 cost= 1.704677287\n",
      "Epoch: 0096 cost= 1.703417985\n",
      "Epoch: 0097 cost= 1.703504301\n",
      "Epoch: 0098 cost= 1.702183129\n",
      "Epoch: 0099 cost= 1.701113971\n",
      "Epoch: 0100 cost= 1.700791257\n",
      "Epoch: 0101 cost= 1.699340640\n",
      "Epoch: 0102 cost= 1.700199454\n",
      "Epoch: 0103 cost= 1.697247927\n",
      "Epoch: 0104 cost= 1.697443627\n",
      "Epoch: 0105 cost= 1.697209353\n",
      "Epoch: 0106 cost= 1.696220600\n",
      "Epoch: 0107 cost= 1.695738273\n",
      "Epoch: 0108 cost= 1.693705147\n",
      "Epoch: 0109 cost= 1.692546339\n",
      "Epoch: 0110 cost= 1.692831498\n",
      "Epoch: 0111 cost= 1.692217243\n",
      "Epoch: 0112 cost= 1.691565159\n",
      "Epoch: 0113 cost= 1.690251912\n",
      "Epoch: 0114 cost= 1.690274678\n",
      "Epoch: 0115 cost= 1.690717615\n",
      "Epoch: 0116 cost= 1.688795969\n",
      "Epoch: 0117 cost= 1.689232680\n",
      "Epoch: 0118 cost= 1.687482572\n",
      "Epoch: 0119 cost= 1.687628313\n",
      "Epoch: 0120 cost= 1.686889955\n",
      "Epoch: 0121 cost= 1.684697874\n",
      "Epoch: 0122 cost= 1.684467402\n",
      "Epoch: 0123 cost= 1.684165969\n",
      "Epoch: 0124 cost= 1.683757057\n",
      "Epoch: 0125 cost= 1.682955184\n",
      "Epoch: 0126 cost= 1.681662257\n",
      "Epoch: 0127 cost= 1.681249553\n",
      "Epoch: 0128 cost= 1.680999715\n",
      "Epoch: 0129 cost= 1.679805375\n",
      "Epoch: 0130 cost= 1.678941897\n",
      "Epoch: 0131 cost= 1.678882165\n",
      "Epoch: 0132 cost= 1.677400994\n",
      "Epoch: 0133 cost= 1.676652663\n",
      "Epoch: 0134 cost= 1.677561097\n",
      "Epoch: 0135 cost= 1.676382123\n",
      "Epoch: 0136 cost= 1.675770596\n",
      "Epoch: 0137 cost= 1.674376344\n",
      "Epoch: 0138 cost= 1.674332219\n",
      "Epoch: 0139 cost= 1.674091954\n",
      "Epoch: 0140 cost= 1.673555376\n",
      "Epoch: 0141 cost= 1.672873210\n",
      "Epoch: 0142 cost= 1.670722334\n",
      "Epoch: 0143 cost= 1.671345067\n",
      "Epoch: 0144 cost= 1.669812265\n",
      "Epoch: 0145 cost= 1.669636261\n",
      "Epoch: 0146 cost= 1.669078591\n",
      "Epoch: 0147 cost= 1.668957688\n",
      "Epoch: 0148 cost= 1.667794873\n",
      "Epoch: 0149 cost= 1.667791192\n",
      "Epoch: 0150 cost= 1.668013728\n",
      "Epoch: 0151 cost= 1.665717621\n",
      "Epoch: 0152 cost= 1.666292769\n",
      "Epoch: 0153 cost= 1.665450691\n",
      "Epoch: 0154 cost= 1.664559153\n",
      "Epoch: 0155 cost= 1.664276803\n",
      "Epoch: 0156 cost= 1.664548517\n",
      "Epoch: 0157 cost= 1.662575000\n",
      "Epoch: 0158 cost= 1.663493259\n",
      "Epoch: 0159 cost= 1.661635626\n",
      "Epoch: 0160 cost= 1.661542472\n",
      "Epoch: 0161 cost= 1.660475909\n",
      "Epoch: 0162 cost= 1.661872321\n",
      "Epoch: 0163 cost= 1.659933098\n",
      "Epoch: 0164 cost= 1.658497092\n",
      "Epoch: 0165 cost= 1.658896066\n",
      "Epoch: 0166 cost= 1.658346865\n",
      "Epoch: 0167 cost= 1.659408195\n",
      "Epoch: 0168 cost= 1.657165719\n",
      "Epoch: 0169 cost= 1.656664040\n",
      "Epoch: 0170 cost= 1.656916228\n",
      "Epoch: 0171 cost= 1.656327486\n",
      "Epoch: 0172 cost= 1.654346953\n",
      "Epoch: 0173 cost= 1.654624159\n",
      "Epoch: 0174 cost= 1.656738405\n",
      "Epoch: 0175 cost= 1.653947805\n",
      "Epoch: 0176 cost= 1.653677826\n",
      "Epoch: 0177 cost= 1.653376225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0178 cost= 1.653007704\n",
      "Epoch: 0179 cost= 1.652282095\n",
      "Epoch: 0180 cost= 1.651369607\n",
      "Epoch: 0181 cost= 1.651901312\n",
      "Epoch: 0182 cost= 1.651163033\n",
      "Epoch: 0183 cost= 1.649752087\n",
      "Epoch: 0184 cost= 1.650235619\n",
      "Epoch: 0185 cost= 1.649188660\n",
      "Epoch: 0186 cost= 1.650372425\n",
      "Epoch: 0187 cost= 1.649189524\n",
      "Epoch: 0188 cost= 1.648355809\n",
      "Epoch: 0189 cost= 1.648500033\n",
      "Epoch: 0190 cost= 1.647651020\n",
      "Epoch: 0191 cost= 1.647671967\n",
      "Epoch: 0192 cost= 1.646251152\n",
      "Epoch: 0193 cost= 1.646073609\n",
      "Epoch: 0194 cost= 1.646503404\n",
      "Epoch: 0195 cost= 1.644283557\n",
      "Epoch: 0196 cost= 1.645797275\n",
      "Epoch: 0197 cost= 1.645083378\n",
      "Epoch: 0198 cost= 1.645409577\n",
      "Epoch: 0199 cost= 1.643638927\n",
      "Epoch: 0200 cost= 1.644030104\n",
      "Finished! \n"
     ]
    }
   ],
   "source": [
    "# Maxout网络的构建方法: 通过reduce_max函数对多个神经元的输出来计算Max值\n",
    "# 将Max值作为输入按照神经元正反向传播进行计算\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import pylab\n",
    "\n",
    "# 导入数据\n",
    "mnist = input_data.read_data_sets('MNIST_data/', one_hot=True)\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# 定义占位符\n",
    "x = tf.placeholder(tf.float32, shape=[None, 28*28]) \n",
    "y = tf.placeholder(tf.float32, shape=[None, 10]) \n",
    "\n",
    "# 设置学习参数\n",
    "W = tf.Variable(tf.random_normal(([784, 10])), name='weight')\n",
    "b = tf.Variable(tf.zeros(([10])), name='bias')\n",
    "z = tf.matmul(x, W) + b\n",
    "\n",
    "# 添加一层Maxout\n",
    "maxout = tf.reduce_max(z, axis=1, keep_dims=True)\n",
    "\n",
    "W2 = tf.Variable(tf.truncated_normal([1, 10], stddev=0.1))\n",
    "b2 = tf.Variable(tf.zeros([1]))\n",
    "\n",
    "# 构建模型\n",
    "pred = tf.nn.softmax(tf.matmul(maxout, W2) + b2)\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(y * tf.log(pred), reduction_indices=1))\n",
    "learning_rate = 0.04\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "training_epochs = 200\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer()) # Initializing \n",
    "    \n",
    "    # 启动循环，开始训练\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # 循环所有数据集\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x:batch_xs, y:batch_ys})\n",
    "        # 计算平均loss值\n",
    "            avg_cost += c / total_batch\n",
    "    # 显示训练中的详细信息\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print('Epoch:', '%04d' % (epoch+1), 'cost=', '{:.9f}'.format(avg_cost))\n",
    "        \n",
    "    print('Finished! ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
